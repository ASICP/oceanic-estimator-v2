# Technical Resources on Domain-Specific Language Models (DSLMs)

A comprehensive guide to building **Domain-Specific Language Models** â€” taking base SLMs like **Gemma**, **Phi**, **Mistral 7B**, and **Llama 3.2** and transforming them into expert systems for specialized fields like healthcare, finance, legal, and more.

This guide covers:
- **Part 1**: SLM Architecture Fundamentals (the foundation you'll build on)
- **Part 2**: Domain Specialization Techniques (how to create DSLMs)
- **Part 3**: Practical Implementation Guides
- **Part 4**: Model Comparisons & Use Cases

---

## Part 1: Architecture Overview for Newcomers

### What Are Small Language Models (SLMs)?

Small Language Models are scaled-down versions of Large Language Models (LLMs), typically ranging from **1 billion to ~14 billion parameters**. They're designed to balance performance with efficiencyâ€”offering lower latency, reduced memory footprint, and the ability to run on edge devices or consumer hardware while still achieving competitive results on many tasks.

**Core advantages over larger models:**
- Lower computational requirements (can run on single GPUs or even CPUs)
- Faster inference times
- Reduced deployment costs
- Ability to run locally for privacy-sensitive applications
- Easier to fine-tune on domain-specific data

### Shared Architectural Foundations

All four model families (Gemma, Phi, Mistral, Llama 3.2 small variants) are **decoder-only transformer architectures**â€”the same fundamental design that powers GPT models. Key shared components include:

| Component | Description |
|-----------|-------------|
| **Decoder-only Transformer** | Generates text autoregressively (predicts next token based on previous tokens) |
| **Rotary Position Embeddings (RoPE)** | Encodes positional information; enables better handling of varying sequence lengths |
| **Grouped-Query Attention (GQA)** | Memory-efficient attention that shares key/value heads across multiple query heads |
| **RMSNorm** | Simpler, faster normalization than LayerNorm |
| **SwiGLU / GeGLU Activations** | Non-linear activation functions that improve model expressiveness |

---

### Model-Specific Architecture Deep Dives

#### ğŸ”· Google Gemma 2

**Key Specs:**
- Sizes: 2B, 9B, 27B parameters
- Context length: 8,192 tokens
- Embedding dimension: Varies by size (e.g., 2048 for 2B, 4096 for 9B)

**Architectural Innovations:**
1. **Alternating Local/Global Attention**: Gemma 2 interleaves sliding window attention (local, efficient) with global attention across layersâ€”balancing computational cost with long-range understanding
2. **Knowledge Distillation**: The 2B and 9B models were trained using logits from the 27B model as targets (not just next-token prediction), significantly boosting performance
3. **Logit Soft-Capping**: Prevents overconfident predictions during training, improving generalization
4. **Model Merging (Warp)**: Post-training technique combining multiple model checkpoints via EMA, SLERP, and linear interpolation

**Resources:**
- [Gemma 2 Technical Report (arXiv)](https://arxiv.org/abs/2408.00118)
- [Gemma Architecture Overview - Google Developers Blog](https://developers.googleblog.com/en/gemma-explained-overview-gemma-model-family-architectures/)
- [Deep Dive with PyTorch Implementation](https://amaarora.github.io/posts/2024-07-07%20Gemma.html)

---

#### ğŸ”· Microsoft Phi Family (Phi-3, Phi-3.5, Phi-4)

**Key Specs:**
- Phi-3-mini: 3.8B parameters (4K and 128K context variants)
- Phi-3-small: 7B parameters
- Phi-3-medium: 14B parameters
- Phi-4: 14B parameters (latest, focused on STEM reasoning)

**Architectural Innovations:**
1. **Data Quality over Scale**: Phi models emphasize synthetic, high-quality training data ("textbook-quality") over raw data volume
2. **Blocksparse Attention** (Phi-3-small): Custom sparse attention pattern that divides context across heads, reducing KV cache requirements
3. **Grouped-Query Attention**: 4 queries share 1 key for memory efficiency
4. **Long Context via LongRope**: Extended context handling up to 128K tokens

**Training Philosophy:**
Phi models prove that data quality can compensate for parameter count. They use curated datasets including:
- Synthetic "textbook-like" data for math, coding, reasoning
- Filtered web documents at appropriate knowledge levels
- High-quality chat data for instruction-following

**Resources:**
- [Phi-4 Technical Report - Microsoft Research](https://www.microsoft.com/en-us/research/publication/phi-4-technical-report/)
- [Phi-3 Technical Report (arXiv PDF)](https://arxiv.org/pdf/2404.14219)
- [Phi-3 Tutorial - DataCamp](https://www.datacamp.com/tutorial/phi-3-tutorial)
- [Phi Models Overview - Azure](https://azure.microsoft.com/en-us/products/phi)

---

#### ğŸ”· Mistral 7B

**Key Specs:**
- Parameters: 7.3 billion
- Context length: 8,192 tokens (32K with sliding window)
- 32 transformer layers, 32 attention heads
- Embedding dimension: 4,096

**Architectural Innovations:**
1. **Sliding Window Attention (SWA)**: Each layer attends only to the previous 4,096 tokens, enabling O(sliding_window Ã— seq_len) complexity instead of O(seq_lenÂ²)
2. **Rolling Buffer KV Cache**: Fixed-size cache that "rolls over" for memory-efficient long sequences
3. **Grouped-Query Attention (GQA)**: Accelerates inference and reduces memory during decoding
4. **Pre-fill and Chunking**: Optimized attention computation during prompt processing

**Why It's Efficient:**
The combination of SWA and stacked transformer layers means that even with a 4K window, information can theoretically propagate across ~131K tokens (4096 Ã— 32 layers) through the network.

**Resources:**
- [Mistral 7B Paper (arXiv)](https://arxiv.org/abs/2310.06825)
- [Official Mistral AI Announcement](https://mistral.ai/news/announcing-mistral-7b)
- [Mistral 7B Explained - Towards Data Science](https://towardsdatascience.com/mistral-7b-explained-towards-more-efficient-language-models-7f9c6e6b7251/)

---

#### ğŸ”· Llama 3.2 (1B & 3B)

**Key Specs:**
- Sizes: 1B and 3B parameters
- Context length: 128K tokens
- Optimized for edge/mobile deployment
- 8 officially supported languages

**Architectural Innovations:**
1. **Pruning + Distillation Pipeline**: Started from Llama 3.1 8B, pruned to reduce size, then used knowledge distillation from 8B and 70B models to recover performance
2. **Quantization-Aware Training (QAT)**: Models also available in 4-bit quantized versions (QAT + LoRA and SpinQuant variants)
3. **Auto-regressive Transformer**: Standard decoder architecture optimized for efficiency
4. **Mobile Optimization**: Designed to run on ExecuTorch for iOS/Android deployment

**Training Process:**
```
Llama 3.1 8B â†’ Pruning â†’ Knowledge Distillation (from 8B/70B) â†’ SFT â†’ DPO â†’ Final 1B/3B models
```

**Resources:**
- [Llama 3.2 Model Cards](https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_2/)
- [Meta AI Blog - Llama 3.2 Release](https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/)
- [Llama 3.2 Guide - DataCamp](https://www.datacamp.com/blog/llama-3-2)
- [HuggingFace - Llama 3.2 1B](https://huggingface.co/meta-llama/Llama-3.2-1B)

---

## Part 2: Domain Specialization â€” Creating DSLMs

### What is a Domain-Specific Language Model (DSLM)?

A **Domain-Specific Language Model (DSLM)** is an AI model trained or fine-tuned on data, terminology, and context unique to a specific industry, organization, or task. Unlike general-purpose LLMs trained on broad internet data, DSLMs deliver:

- **Higher accuracy** on domain tasks (fewer hallucinations)
- **Better contextual understanding** of specialized terminology
- **Compliance alignment** with industry regulations (HIPAA, GDPR, financial regulations)
- **Reduced errors** in high-stakes applications (medical diagnosis, legal analysis, financial advice)

> **Gartner predicts**: More than 60% of generative AI models used by enterprises will be domain-specific by 2028.

---

### The DSLM Landscape: Notable Examples

| Domain | Model | Base Model | Key Features |
|--------|-------|------------|--------------|
| **Medical** | BioMistral | Mistral 7B | Pre-trained on PubMed Central; multilingual medical QA |
| **Medical** | Med-PaLM 2 | PaLM | Physician-level accuracy on USMLE; clinical decision support |
| **Medical** | PMC-LLaMA | LLaMA | Trained on PubMed Central papers |
| **Medical** | MediTron | LLaMA | Open-source medical LLM |
| **Finance** | BloombergGPT | Custom 50B | Trained on 50B+ financial documents |
| **Finance** | FinGPT | Various (6B/7B) | Open-source; RLHF with stock prices |
| **Finance** | Palmyra-Fin 70B | Custom | CFA exam passing; financial analysis |
| **Legal** | ChatLAW | Custom | Contract analysis, case law summarization |
| **Legal** | SaulLM | Mistral | Legal document processing |
| **Climate** | ClimateBERT | BERT | Climate-related text analysis |
| **Code** | CodeGemma | Gemma | Code completion and generation |

---

### Three Approaches to Building a DSLM

There are three main strategies to create a domain expert from a base SLM, each with different trade-offs:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DSLM Creation Approaches                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚  1. RAG (Retrieval-Augmented Generation)                                â”‚
â”‚     â””â”€â”€ Keep model unchanged; augment with external knowledge           â”‚
â”‚                                                                          â”‚
â”‚  2. Fine-Tuning (SFT, LoRA, QLoRA)                                      â”‚
â”‚     â””â”€â”€ Adapt model weights on domain-specific datasets                 â”‚
â”‚                                                                          â”‚
â”‚  3. Continued Pre-Training (CPT)                                        â”‚
â”‚     â””â”€â”€ Further pre-train on large domain corpus (most expensive)       â”‚
â”‚                                                                          â”‚
â”‚  4. Hybrid (RAG + Fine-Tuning) â† RECOMMENDED for most use cases         â”‚
â”‚     â””â”€â”€ Fine-tune for domain style/reasoning + RAG for current facts    â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Approach 1: RAG (Retrieval-Augmented Generation)

**What it does**: Connects the LLM to external knowledge bases (vector databases, documents) at inference time. The model retrieves relevant information and uses it as context.

**Best for**:
- Frequently updated information (policies, regulations, product catalogs)
- When you need source citations and traceability
- Privacy-sensitive data (data stays in your database, not baked into model)
- Multi-domain applications (swap knowledge bases without retraining)

**Not ideal for**:
- Teaching new reasoning patterns or domain-specific logic
- Changing the model's tone, style, or output format
- Tasks requiring deep internalization of domain terminology

**Architecture Overview**:
```
User Query â†’ Embedding â†’ Vector Search â†’ Retrieve Top-K Documents
                                              â†“
                              LLM + Retrieved Context â†’ Response
```

**Key Components**:
- **Embedding Model**: Converts text to vectors (e.g., `text-embedding-3-small`, `bge-large`)
- **Vector Database**: Stores and searches embeddings (Pinecone, Weaviate, Chroma, pgvector)
- **Retriever**: Finds relevant documents (semantic search, hybrid search)
- **Generator**: The base LLM that produces the final response

**Resources**:
- [RAG vs Fine-tuning: Pipelines, Tradeoffs (arXiv)](https://arxiv.org/abs/2401.08406)
- [IBM: RAG vs Fine-tuning](https://www.ibm.com/think/topics/rag-vs-fine-tuning)
- [DataCamp: RAG vs Fine-Tuning Tutorial](https://www.datacamp.com/tutorial/rag-vs-fine-tuning)

---

### Approach 2: Fine-Tuning

**What it does**: Adjusts the model's internal weights using domain-specific training data. The knowledge becomes "baked into" the model.

**Best for**:
- Teaching domain-specific terminology and reasoning patterns
- Changing output style, tone, or format consistently
- Tasks requiring deep domain expertise (medical diagnosis, legal analysis)
- Reducing inference latency (no retrieval step)
- Deploying to edge devices where RAG infrastructure isn't feasible

**Not ideal for**:
- Rapidly changing information (requires retraining)
- When you need source citations
- Very large knowledge bases (can't fit everything in model weights)

#### Fine-Tuning Methods Comparison

| Method | What Changes | Training Cost | Memory | Best For |
|--------|--------------|---------------|--------|----------|
| **Full Fine-Tuning** | All parameters | Very High | Very High | Maximum adaptation (rarely needed) |
| **LoRA** | Low-rank adapter matrices | Low | Low | General domain adaptation |
| **QLoRA** | LoRA + 4-bit quantization | Very Low | Very Low | Resource-constrained environments |
| **Adapters** | Small inserted modules | Low | Low | Multi-task scenarios |
| **Prompt Tuning** | Learned prompt embeddings | Very Low | Very Low | Simple task adaptation |

#### The Fine-Tuning Pipeline

```
1. DATA PREPARATION
   â”œâ”€â”€ Collect domain-specific documents/examples
   â”œâ”€â”€ Clean and format into instruction format (question-answer pairs)
   â”œâ”€â”€ Create train/validation splits
   â””â”€â”€ Ensure diversity and coverage of domain topics

2. TRAINING SETUP
   â”œâ”€â”€ Choose base model (Gemma, Phi, Mistral, Llama)
   â”œâ”€â”€ Select fine-tuning method (LoRA/QLoRA recommended)
   â”œâ”€â”€ Configure hyperparameters (learning rate, epochs, batch size)
   â””â”€â”€ Set up evaluation metrics

3. TRAINING
   â”œâ”€â”€ Load model with quantization (4-bit for QLoRA)
   â”œâ”€â”€ Attach LoRA adapters to attention layers
   â”œâ”€â”€ Train on domain dataset
   â””â”€â”€ Monitor loss and validation metrics

4. EVALUATION & ITERATION
   â”œâ”€â”€ Test on held-out domain questions
   â”œâ”€â”€ Check for catastrophic forgetting on general tasks
   â”œâ”€â”€ Iterate on data quality if needed
   â””â”€â”€ Merge adapters into base model (optional)
```

---

### Approach 3: Continued Pre-Training (CPT)

**What it does**: Further pre-trains the base model on a large corpus of domain text using the original self-supervised objective (next-token prediction).

**Best for**:
- Adding entirely new language/terminology the model has never seen
- Domains with very specialized vocabulary (biomedical, legal jargon)
- When you have massive amounts of domain text (billions of tokens)

**Challenges**:
- **Very expensive**: Requires significant compute (days/weeks of GPU time)
- **Catastrophic forgetting**: Model may lose general capabilities
- **Data requirements**: Need large, high-quality domain corpus

**When to Consider CPT**:
```
Use CPT when:
âœ“ Domain vocabulary is highly specialized (medical, scientific)
âœ“ You have >1B tokens of high-quality domain text
âœ“ Fine-tuning alone isn't capturing domain nuances
âœ“ You have substantial compute budget

Skip CPT when:
âœ— Domain language overlaps with general English
âœ— Fine-tuning achieves acceptable performance
âœ— Limited compute resources
âœ— Need to preserve general capabilities
```

**Recommended Approach** (from Meta AI research):
> "For teams with limited resources, we do not recommend continued pre-training. Fine-tuning with parameter-efficient methods (PEFT) is more cost-effective and achieves state-of-the-art results in specialized domains."

---

### The Hybrid Approach: Fine-Tuning + RAG

The most robust domain-specific systems combine both approaches:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    HYBRID DSLM ARCHITECTURE                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                â”‚
â”‚   FINE-TUNING provides:                                        â”‚
â”‚   â€¢ Domain reasoning patterns                                  â”‚
â”‚   â€¢ Specialized terminology understanding                      â”‚
â”‚   â€¢ Consistent output style/format                             â”‚
â”‚   â€¢ Professional tone appropriate to domain                    â”‚
â”‚                                                                â”‚
â”‚   RAG provides:                                                â”‚
â”‚   â€¢ Current, up-to-date information                            â”‚
â”‚   â€¢ Specific facts and figures                                 â”‚
â”‚   â€¢ Source citations for verification                          â”‚
â”‚   â€¢ Organization-specific knowledge                            â”‚
â”‚                                                                â”‚
â”‚   Result: A domain expert that thinks like a professional      â”‚
â”‚           AND has access to current information                â”‚
â”‚                                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Example**: A legal AI assistant
- **Fine-tuned** to understand legal reasoning, cite precedents correctly, use proper legal terminology
- **RAG-augmented** with the firm's case database, current regulations, and client documents

---

### Avoiding Catastrophic Forgetting

A major challenge when fine-tuning is **catastrophic forgetting** â€” the model loses previously learned capabilities when trained on new domain data.

**Research findings**:
- Forgetting increases as a power law with training steps
- Even LoRA/QLoRA are not immune to forgetting
- Larger models can forget more severely (more to lose)

**Mitigation Strategies**:

| Strategy | Description | Effectiveness |
|----------|-------------|---------------|
| **LoRA/QLoRA** | Only train small adapter weights | Moderate (not a complete solution) |
| **Data Mixing** | Include general data alongside domain data | High |
| **Rehearsal** | Periodically train on original capabilities | High |
| **EWC (Elastic Weight Consolidation)** | Penalize changes to important weights | Moderate |
| **Model Merging** | Merge domain-adapted model with original | High |
| **Early Stopping** | Stop before excessive forgetting | Low (doesn't prevent, just limits) |

**Recommended Workflow**:
```python
# 1. Prepare mixed dataset (80% domain, 20% general)
mixed_data = combine_datasets(
    domain_data,      # Your specialized data
    general_data,     # Sample from original training distribution
    ratio=0.8
)

# 2. Fine-tune with LoRA
model = fine_tune_with_lora(base_model, mixed_data)

# 3. Evaluate on BOTH domain AND general benchmarks
domain_score = evaluate(model, domain_test_set)
general_score = evaluate(model, mmlu_sample)  # Check for forgetting

# 4. If general capabilities dropped significantly, consider model merging
if general_score < threshold:
    model = merge_models(fine_tuned_model, base_model, ratio=0.7)
```

**Resources**:
- [Scaling Laws for Forgetting When Fine-Tuning LLMs (arXiv)](https://arxiv.org/abs/2401.05605)
- [Catastrophic Forgetting in LLMs During Continual Fine-tuning (arXiv)](https://arxiv.org/abs/2308.08747)
- [Awesome Forgetting in Deep Learning (GitHub)](https://github.com/EnnengYang/Awesome-Forgetting-in-Deep-Learning)

---

### Decision Framework: RAG vs Fine-Tuning vs Hybrid

```
                    START HERE
                         â”‚
                         â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Does info change frequently?   â”‚
        â”‚ (daily/weekly updates)         â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚           â”‚
                   YES          NO
                    â”‚           â”‚
                    â–¼           â–¼
               Use RAG    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚ Need to change model's  â”‚
                          â”‚ reasoning/style/format? â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚           â”‚
                                  YES          NO
                                   â”‚           â”‚
                                   â–¼           â–¼
                            Fine-Tune    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                         â”‚ Need citations/ â”‚
                                         â”‚ traceability?   â”‚
                                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                               â”‚        â”‚
                                              YES       NO
                                               â”‚        â”‚
                                               â–¼        â–¼
                                           Use RAG   Either works,
                                                     RAG is simpler
```

**Quick Reference Table**:

| Requirement | RAG | Fine-Tuning | Hybrid |
|-------------|-----|-------------|--------|
| Up-to-date information | âœ… Best | âŒ Stale | âœ… |
| Source citations | âœ… Best | âŒ No | âœ… |
| Domain reasoning | âŒ Limited | âœ… Best | âœ… |
| Custom output style | âŒ No | âœ… Best | âœ… |
| Privacy (data not in model) | âœ… Best | âŒ In weights | âœ… |
| Low latency | âŒ Retrieval overhead | âœ… Best | âš ï¸ Trade-off |
| Edge/offline deployment | âŒ Needs DB | âœ… Best | âš ï¸ Complex |
| Multi-domain scaling | âœ… Swap DBs | âŒ Need multiple models | âœ… |
| Setup complexity | âš ï¸ Medium | âš ï¸ Medium | âŒ High |

---

### Domain-Specific Data Preparation

**The quality of your domain data is the most important factor in DSLM success.**

#### Data Collection Sources

| Domain | Potential Data Sources |
|--------|------------------------|
| **Medical** | PubMed, clinical notes (de-identified), medical textbooks, FDA documents |
| **Legal** | Case law databases, contracts, regulatory filings, legal journals |
| **Finance** | SEC filings, earnings calls, financial news, analyst reports |
| **Technical** | Documentation, Stack Overflow, GitHub issues, internal wikis |
| **Customer Support** | Ticket histories, chat logs, FAQ databases, product manuals |

#### Data Formatting for Instruction Tuning

Convert your domain data into instruction-following format:

```json
{
  "messages": [
    {"role": "system", "content": "You are a medical assistant specializing in cardiology."},
    {"role": "user", "content": "What are the contraindications for beta-blockers?"},
    {"role": "assistant", "content": "Beta-blockers are contraindicated in patients with: 1) Severe bradycardia... [detailed medical response]"}
  ]
}
```

#### Data Quality Checklist

- [ ] **Coverage**: Does data cover the full range of domain topics?
- [ ] **Accuracy**: Has domain expert validated correctness?
- [ ] **Diversity**: Are different phrasings and scenarios represented?
- [ ] **Format consistency**: Is instruction format uniform?
- [ ] **Deduplication**: Are near-duplicates removed?
- [ ] **Balance**: Are topics/categories roughly balanced?
- [ ] **Recency**: Is information current and not outdated?

#### Data Augmentation Techniques

When domain data is scarce:
1. **Paraphrasing**: Use LLMs to rephrase existing examples
2. **Synthetic generation**: Have GPT-4 generate additional Q&A pairs (then validate)
3. **Back-translation**: Translate to another language and back
4. **Template-based**: Create variations using templates with slot-filling

---

### Evaluation for Domain-Specific Models

**Don't just use general benchmarks â€” create domain-specific evaluations.**

#### Evaluation Framework

```
1. DOMAIN-SPECIFIC ACCURACY
   â”œâ”€â”€ Create held-out test set from domain data
   â”œâ”€â”€ Have domain experts rate response quality
   â””â”€â”€ Measure terminology usage correctness

2. GENERAL CAPABILITY RETENTION
   â”œâ”€â”€ Test on MMLU subset (check for forgetting)
   â”œâ”€â”€ Test on general reasoning tasks
   â””â”€â”€ Compare to base model baseline

3. SAFETY & COMPLIANCE
   â”œâ”€â”€ Test for hallucination rates
   â”œâ”€â”€ Verify regulatory compliance
   â””â”€â”€ Check for harmful outputs in domain context

4. REAL-WORLD PERFORMANCE
   â”œâ”€â”€ A/B test with actual users
   â”œâ”€â”€ Measure task completion rates
   â””â”€â”€ Track user satisfaction/feedback
```

#### Domain-Specific Benchmarks

| Domain | Benchmarks |
|--------|------------|
| **Medical** | MedQA, PubMedQA, USMLE-style questions, MedMCQA |
| **Legal** | LegalBench, CaseHOLD, contract analysis tasks |
| **Finance** | FinQA, financial sentiment analysis, CFA exam questions |
| **Code** | HumanEval, MBPP, domain-specific code tasks |

---

## Part 3: Practical Implementation Guides

### Getting Started Checklist

| Step | Description | Tools |
|------|-------------|-------|
| 1. Choose your model | Match model size to your hardware and use case | See comparison table below |
| 2. Set up environment | Install inference framework | Ollama, vLLM, llama.cpp, Transformers |
| 3. Download weights | Get model from official sources | HuggingFace, Kaggle, official repos |
| 4. Run inference | Test with basic prompts | Python script or CLI |
| 5. (Optional) Fine-tune | Adapt to your domain | LoRA, QLoRA, Unsloth |

---

### Inference Frameworks

**For Local/Development:**
- **Ollama**: Easiest setup for local inference ([ollama.com](https://ollama.com))
- **llama.cpp / Gemma.cpp**: CPU-optimized inference
- **LM Studio**: GUI for local model management

**For Production:**
- **vLLM**: High-throughput serving with PagedAttention
- **TensorRT-LLM**: NVIDIA-optimized inference
- **Text Generation Inference (TGI)**: HuggingFace's serving solution

**Example: Running Gemma 2 with Ollama**
```bash
# Install Ollama
curl -fsSL https://ollama.com/install.sh | sh

# Pull and run Gemma 2 9B
ollama pull gemma2:9b
ollama run gemma2:9b
```

**Example: Running Phi-3 with Transformers**
```python
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

model_id = "microsoft/Phi-3-mini-4k-instruct"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.float16,
    device_map="auto"
)

messages = [{"role": "user", "content": "Explain transformers in simple terms"}]
inputs = tokenizer.apply_chat_template(messages, return_tensors="pt").to(model.device)
outputs = model.generate(inputs, max_new_tokens=256)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

---

### Fine-Tuning Guide

**When to Fine-Tune:**
- Model lacks domain-specific knowledge (legal, medical, technical)
- Need consistent output formatting
- Specific task requires specialized behavior
- Base model accuracy is insufficient

**Efficient Fine-Tuning Techniques:**

| Technique | Description | Memory Savings |
|-----------|-------------|----------------|
| **LoRA** | Trains small "adapter" matrices instead of full weights | ~90% |
| **QLoRA** | LoRA + 4-bit quantized base model | ~95% |
| **Full Fine-Tuning** | Updates all parameters | 0% (baseline) |

**Recommended Stack:**
- **Unsloth**: 2x faster training, 70% less VRAM
- **HuggingFace TRL**: SFTTrainer, DPO trainer
- **PEFT**: Parameter-efficient fine-tuning library

**Example: Fine-tuning Llama 3.2 3B with QLoRA**
```python
from unsloth import FastLanguageModel
from trl import SFTTrainer

# Load model with 4-bit quantization
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/Llama-3.2-3B-Instruct",
    max_seq_length=2048,
    load_in_4bit=True,
)

# Add LoRA adapters
model = FastLanguageModel.get_peft_model(
    model,
    r=16,  # LoRA rank
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    lora_alpha=16,
    lora_dropout=0,
)

# Train with SFTTrainer
trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=your_dataset,
    max_seq_length=2048,
)
trainer.train()
```

**Fine-Tuning Resources:**
- [Fine-tuning SLMs Guide - Omdena](https://www.omdena.com/blog/fine-tuning-small-language-models)
- [Phi-3 Fine-tuning on Azure - Microsoft](https://techcommunity.microsoft.com/blog/machinelearningblog/finetune-small-language-model-slm-phi-3-using-azure-machine-learning/4130399)
- [SLM Fine-tuning for Function Calling - Microsoft](https://techcommunity.microsoft.com/blog/machinelearningblog/fine-tuning-small-language-models-for-function-calling-a-comprehensive-guide/4362539)
- [Unsloth Documentation](https://docs.unsloth.ai/)
- [NVIDIA Blog: Fine-tuning SLMs for Code Review](https://developer.nvidia.com/blog/fine-tuning-small-language-models-to-optimize-code-review-accuracy/)

---

### Hardware Requirements

| Model | GPU VRAM (FP16) | GPU VRAM (4-bit) | Suitable Hardware |
|-------|-----------------|------------------|-------------------|
| Llama 3.2 1B | ~2 GB | ~1 GB | Any modern GPU, Apple M1+, CPU |
| Llama 3.2 3B | ~6 GB | ~2 GB | RTX 3060+, Apple M1+ |
| Gemma 2 2B | ~4 GB | ~1.5 GB | Any modern GPU |
| Phi-3-mini (3.8B) | ~8 GB | ~3 GB | RTX 3060+, Apple M1+ |
| Mistral 7B | ~14 GB | ~4 GB | RTX 3080+, RTX 4070+ |
| Gemma 2 9B | ~18 GB | ~6 GB | RTX 3090+, RTX 4080+ |
| Phi-4 (14B) | ~28 GB | ~8 GB | RTX 4090, A100 |
| Gemma 2 27B | ~54 GB | ~16 GB | A100 80GB, H100 |

---

## Part 4: Model Comparison Table

### Architecture & Training Comparison

| Feature | Gemma 2 (9B) | Phi-3-mini (3.8B) | Mistral 7B | Llama 3.2 (3B) |
|---------|--------------|-------------------|------------|----------------|
| **Parameters** | 9B | 3.8B | 7.3B | 3B |
| **Context Length** | 8K | 4K / 128K | 8K (32K with SWA) | 128K |
| **Attention Type** | GQA + Local/Global alternating | GQA | GQA + Sliding Window | GQA |
| **Training Tokens** | ~13T | 3.3T | Not disclosed | 9T |
| **Knowledge Distillation** | Yes (from 27B) | Yes (from GPT-4) | No | Yes (from 8B/70B) |
| **Position Encoding** | RoPE | RoPE | RoPE | RoPE |
| **Activation** | GeGLU | SwiGLU | SiLU | SwiGLU |
| **Normalization** | RMSNorm (pre + post) | RMSNorm | RMSNorm | RMSNorm |
| **License** | Gemma License (commercial OK) | MIT | Apache 2.0 | Llama 3.2 Community License |

---

### Benchmark Performance Comparison

| Benchmark | Gemma 2 9B | Phi-3-mini (3.8B) | Mistral 7B | Llama 3.2 3B | Notes |
|-----------|------------|-------------------|------------|--------------|-------|
| **MMLU** (5-shot) | ~71% | ~69% | ~60% | ~63% | General knowledge across 57 topics |
| **HellaSwag** | ~82% | ~77% | ~81% | ~76% | Common sense reasoning |
| **GSM8K** (math) | ~70% | ~75% | ~52% | ~77% | Grade-school math problems |
| **HumanEval** (code) | ~54% | ~58% | ~30% | ~33% | Code generation |
| **ARC-Challenge** | ~68% | ~85% | ~63% | ~78% | Science reasoning |
| **TruthfulQA** | ~52% | ~54% | ~42% | ~52% | Factual accuracy |

*Note: Benchmark scores vary by evaluation methodology. These are approximate values from official reports and third-party evaluations.*

---

### Practical Use Case Recommendations

| Use Case | Best Choice | Why |
|----------|-------------|-----|
| **Mobile/Edge deployment** | Llama 3.2 1B/3B | Optimized for ExecuTorch, smallest footprint |
| **Long document processing** | Phi-3.5-mini (128K) or Llama 3.2 | Native 128K context support |
| **Math/STEM reasoning** | Phi-3/Phi-4 | Trained specifically on synthetic STEM data |
| **Code generation** | Phi-3 or Gemma 2 | Strong HumanEval scores |
| **General chat/assistant** | Gemma 2 9B or Mistral 7B | Balanced capabilities, good instruction following |
| **Multilingual** | Llama 3.2 or Phi-3.5-MoE | 8+ officially supported languages |
| **Resource-constrained server** | Mistral 7B | Excellent efficiency-to-capability ratio |
| **Research/experimentation** | Gemma 2 | Strong documentation, Keras integration |

---

### Ecosystem & Tooling Support

| Model Family | HuggingFace | Ollama | vLLM | llama.cpp | TensorRT | Cloud APIs |
|--------------|-------------|--------|------|-----------|----------|------------|
| Gemma 2 | âœ… | âœ… | âœ… | âœ… (via Gemma.cpp) | âœ… | Google AI Studio, Vertex AI |
| Phi-3/4 | âœ… | âœ… | âœ… | âœ… | âœ… | Azure AI Studio |
| Mistral 7B | âœ… | âœ… | âœ… | âœ… | âœ… | Mistral API, AWS, Azure |
| Llama 3.2 | âœ… | âœ… | âœ… | âœ… | âœ… | AWS Bedrock, Azure, Groq |

---

## Additional Resources

### Official Documentation
- [Google Gemma Docs](https://ai.google.dev/gemma)
- [Microsoft Phi Cookbook](https://github.com/microsoft/Phi-3CookBook)
- [Mistral AI Documentation](https://docs.mistral.ai/)
- [Meta Llama Documentation](https://www.llama.com/docs/)

### Research Papers â€” Base Models
- Gemma 2: [arXiv:2408.00118](https://arxiv.org/abs/2408.00118)
- Phi-3: [arXiv:2404.14219](https://arxiv.org/abs/2404.14219)
- Mistral 7B: [arXiv:2310.06825](https://arxiv.org/abs/2310.06825)

### Research Papers â€” Domain Specialization
- BioMistral (Medical): [arXiv:2402.10373](https://arxiv.org/abs/2402.10373)
- Fine-tuning for Domain-specific MT: [arXiv:2402.15061](https://arxiv.org/abs/2402.15061)
- RAG vs Fine-tuning Tradeoffs: [arXiv:2401.08406](https://arxiv.org/abs/2401.08406)
- Catastrophic Forgetting in LLMs: [arXiv:2308.08747](https://arxiv.org/abs/2308.08747)
- Scaling Laws for Forgetting: [arXiv:2401.05605](https://arxiv.org/abs/2401.05605)
- LLMs for Finance, Healthcare, Law (Survey): [arXiv:2405.01769](https://arxiv.org/abs/2405.01769)
- Fine-tuning for Domain Adaptation (Nature): [Nature Computational Materials](https://www.nature.com/articles/s41524-025-01564-y)

### Tutorials & Courses
- [DataCamp: Phi-3 Tutorial](https://www.datacamp.com/tutorial/phi-3-tutorial)
- [DataCamp: Llama 3.2 Guide](https://www.datacamp.com/blog/llama-3-2)
- [DataCamp: RAG vs Fine-Tuning Tutorial](https://www.datacamp.com/tutorial/rag-vs-fine-tuning)
- [Analytics Vidhya: Fine-tuning SLMs](https://www.analyticsvidhya.com/blog/2024/09/fine-tuning-inference-of-small-language-models-like-gemma/)
- [HuggingFace: SLM Overview](https://huggingface.co/blog/jjokah/small-language-model)
- [Omdena: Fine-tuning SLMs Guide](https://www.omdena.com/blog/fine-tuning-small-language-models)
- [Microsoft: Fine-tuning Phi-3 on Azure](https://techcommunity.microsoft.com/blog/machinelearningblog/finetune-small-language-model-slm-phi-3-using-azure-machine-learning/4130399)
- [NVIDIA: Fine-tuning SLMs for Code Review](https://developer.nvidia.com/blog/fine-tuning-small-language-models-to-optimize-code-review-accuracy/)
- [Together AI: Continued Fine-tuning Deep Dive](https://www.together.ai/blog/continued-fine-tuning)
- [Meta AI: Methods for Adapting LLMs](https://ai.meta.com/blog/adapting-large-language-models-llms/)

### Domain-Specific Model Repositories
- [BioMistral on HuggingFace](https://huggingface.co/BioMistral)
- [FinGPT on GitHub](https://github.com/AI4Finance-Foundation/FinGPT)
- [MedAlpaca on HuggingFace](https://huggingface.co/medalpaca)
- [CodeGemma on HuggingFace](https://huggingface.co/google/codegemma-7b)

### Community & Benchmarks
- [HuggingFace Open LLM Leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard)
- [LM Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness)
- [Chatbot Arena](https://chat.lmsys.org/)
- [Awesome Forgetting in Deep Learning (GitHub)](https://github.com/EnnengYang/Awesome-Forgetting-in-Deep-Learning)

### RAG vs Fine-Tuning Decision Resources
- [IBM: RAG vs Fine-tuning](https://www.ibm.com/think/topics/rag-vs-fine-tuning)
- [Oracle: RAG vs Fine-Tuning Guide](https://www.oracle.com/artificial-intelligence/generative-ai/retrieval-augmented-generation-rag/rag-fine-tuning/)
- [Red Hat: RAG vs Fine-tuning](https://www.redhat.com/en/topics/ai/rag-vs-fine-tuning)
- [SuperAnnotate: Domain-Specific LLMs](https://www.superannotate.com/blog/domain-specific-llms)

---

*Last updated: December 2024*
